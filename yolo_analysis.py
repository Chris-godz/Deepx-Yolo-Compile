#!/usr/bin/env python3
"""
YOLOv5 DXNN vs YOLOv5SU ç›¸ä¼¼åº¦å¯¹æ¯”
è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""

import os
import cv2
import numpy as np
import json
import torch
from dx_engine import InferenceEngine
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

def letterbox_resize(image, target_size=(640, 640), fill_value=114):
    """Letterbox resize with padding"""
    h, w = image.shape[:2]
    target_h, target_w = target_size
    
    r = min(target_h / h, target_w / w)
    new_h, new_w = int(h * r), int(w * r)
    
    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
    padded = np.full((target_h, target_w, 3), fill_value, dtype=np.uint8)
    
    dy = (target_h - new_h) // 2
    dx = (target_w - new_w) // 2
    
    padded[dy:dy+new_h, dx:dx+new_w] = resized
    return padded, r, (dx, dy)

def calculate_similarity_metrics(output1, output2):
    """è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦æŒ‡æ ‡"""
    flat1 = output1.flatten()
    flat2 = output2.flatten()
    
    min_len = min(len(flat1), len(flat2))
    flat1 = flat1[:min_len]
    flat2 = flat2[:min_len]
    
    metrics = {}
    
    # 1. ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)
    metrics['ä½™å¼¦ç›¸ä¼¼åº¦'] = 1 - cosine(flat1, flat2)
    
    # 2. çš®å°”é€Šç›¸å…³ç³»æ•° (Pearson Correlation)
    try:
        pearson_corr, _ = pearsonr(flat1, flat2)
        metrics['çš®å°”é€Šç›¸å…³ç³»æ•°'] = pearson_corr
    except:
        metrics['çš®å°”é€Šç›¸å…³ç³»æ•°'] = 0.0
    
    # 3. å‡æ–¹è¯¯å·® (MSE)
    metrics['å‡æ–¹è¯¯å·®(MSE)'] = mean_squared_error(flat1, flat2)
    
    # 4. å¹³å‡ç»å¯¹è¯¯å·® (MAE)
    metrics['å¹³å‡ç»å¯¹è¯¯å·®(MAE)'] = mean_absolute_error(flat1, flat2)
    
    # 5. å½’ä¸€åŒ–å‡æ–¹æ ¹è¯¯å·® (NRMSE)
    rmse = np.sqrt(metrics['å‡æ–¹è¯¯å·®(MSE)'])
    data_range = np.max(flat1) - np.min(flat1)
    metrics['å½’ä¸€åŒ–RMSE'] = rmse / data_range if data_range != 0 else 0
    
    # 6. ç»“æž„ç›¸ä¼¼æ€§æŒ‡æ•° (SSIM-like)
    mean1, mean2 = np.mean(flat1), np.mean(flat2)
    var1, var2 = np.var(flat1), np.var(flat2)
    covar = np.mean((flat1 - mean1) * (flat2 - mean2))
    
    c1, c2 = 0.01, 0.03
    ssim = ((2 * mean1 * mean2 + c1) * (2 * covar + c2)) / \
           ((mean1**2 + mean2**2 + c1) * (var1 + var2 + c2))
    metrics['ç»“æž„ç›¸ä¼¼æ€§(SSIM)'] = ssim
    
    # 7. ç›¸å¯¹è¯¯å·® (Relative Error)
    metrics['ç›¸å¯¹è¯¯å·®'] = np.mean(np.abs(flat1 - flat2) / (np.abs(flat1) + 1e-8))
    
    return metrics

def create_similarity_visualization(metrics):
    """åˆ›å»ºç›¸ä¼¼åº¦æŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨"""
    
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('YOLOv5 DXNN vs YOLOv5SU Model Similarity Analysis', fontsize=16, fontweight='bold')
    
    # 1. ç›¸ä¼¼åº¦æŒ‡æ ‡æ¡å½¢å›¾
    ax1 = axes[0, 0]
    similarity_metrics = {
        'Cosine Similarity': metrics['ä½™å¼¦ç›¸ä¼¼åº¦'],
        'Pearson Correlation': metrics['çš®å°”é€Šç›¸å…³ç³»æ•°'],
        'SSIM': metrics['ç»“æž„ç›¸ä¼¼æ€§(SSIM)']
    }
    ax1.set_title('Similarity Metrics (Closer to 1 = More Similar)', fontweight='bold')
    ax1.set_ylabel('Similarity Value')
    
    bars = ax1.bar(similarity_metrics.keys(), similarity_metrics.values(), 
                   color=['#2E8B57', '#4169E1', '#FF6347'])
    ax1.set_ylim(0, 1.1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, similarity_metrics.values()):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.6f}', ha='center', va='bottom', fontweight='bold')
    
    # 2. è¯¯å·®æŒ‡æ ‡æ¡å½¢å›¾
    ax2 = axes[0, 1]
    error_metrics = {
        'MSE': metrics['å‡æ–¹è¯¯å·®(MSE)'],
        'MAE': metrics['å¹³å‡ç»å¯¹è¯¯å·®(MAE)'],
        'Normalized RMSE': metrics['å½’ä¸€åŒ–RMSE'],
        'Relative Error': metrics['ç›¸å¯¹è¯¯å·®']
    }
    ax2.set_title('Error Metrics (Closer to 0 = More Similar)', fontweight='bold')
    ax2.set_ylabel('Error Value')
    
    bars = ax2.bar(error_metrics.keys(), error_metrics.values(), 
                   color=['#FF4500', '#FF8C00', '#FFA500', '#FFD700'])
    ax2.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, error_metrics.values()):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(error_metrics.values())*0.01,
                f'{value:.6f}', ha='center', va='bottom', fontweight='bold')
    
    # 3. ç»¼åˆç›¸ä¼¼åº¦è¯„åˆ†é›·è¾¾å›¾
    ax3 = axes[1, 0]
    ax3.remove()  # ç§»é™¤åŽŸè½´
    ax3 = fig.add_subplot(2, 2, 3, projection='polar')
    
    categories = ['Cosine Sim', 'Pearson Corr', 'SSIM', 'Low MSE', 'Low MAE', 'Low NRMSE']
    radar_title = 'Comprehensive Similarity Score\n(Outer Ring = Perfect Similarity)'
    
    values = [
        metrics['ä½™å¼¦ç›¸ä¼¼åº¦'],
        metrics['çš®å°”é€Šç›¸å…³ç³»æ•°'],
        metrics['ç»“æž„ç›¸ä¼¼æ€§(SSIM)'],
        1 - min(metrics['å‡æ–¹è¯¯å·®(MSE)'] / 10, 1),  # åè½¬å¹¶å½’ä¸€åŒ–
        1 - min(metrics['å¹³å‡ç»å¯¹è¯¯å·®(MAE)'] / 2, 1),  # åè½¬å¹¶å½’ä¸€åŒ–
        1 - min(metrics['å½’ä¸€åŒ–RMSE'], 1)  # åè½¬
    ]
    
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    values += values[:1]  # é—­åˆå›¾å½¢
    angles += angles[:1]
    
    ax3.plot(angles, values, 'o-', linewidth=2, color='#1f77b4')
    ax3.fill(angles, values, alpha=0.25, color='#1f77b4')
    ax3.set_xticks(angles[:-1])
    ax3.set_xticklabels(categories)
    ax3.set_ylim(0, 1)
    ax3.set_title(radar_title, pad=20, fontweight='bold')
    ax3.grid(True)
    
    # 4. è´¨é‡è¯„ä¼°è¡¨æ ¼
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    # è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†
    avg_similarity = np.mean([metrics['ä½™å¼¦ç›¸ä¼¼åº¦'], metrics['çš®å°”é€Šç›¸å…³ç³»æ•°'], metrics['ç»“æž„ç›¸ä¼¼æ€§(SSIM)']])
    
    if avg_similarity > 0.99:
        quality = "EXCELLENT"
        quality_desc = "Highly Similar (>99%)"
    elif avg_similarity > 0.95:
        quality = "GOOD"
        quality_desc = "Quite Similar (>95%)"
    elif avg_similarity > 0.90:
        quality = "ACCEPTABLE"
        quality_desc = "Reasonably Similar (>90%)"
    else:
        quality = "POOR"
        quality_desc = "Significant Differences (<90%)"
    
    # åˆ›å»ºè¯„ä¼°è¡¨æ ¼
    table_data = [
        ['Evaluation Item', 'Value', 'Description'],
        ['Cosine Similarity', f'{metrics["ä½™å¼¦ç›¸ä¼¼åº¦"]:.6f}', 'Closer to 1 = More Similar'],
        ['Pearson Correlation', f'{metrics["çš®å°”é€Šç›¸å…³ç³»æ•°"]:.6f}', 'Linear Correlation Degree'],
        ['Structural Similarity', f'{metrics["ç»“æž„ç›¸ä¼¼æ€§(SSIM)"]:.6f}', 'Structure Info Preservation'],
        ['Mean Squared Error', f'{metrics["å‡æ–¹è¯¯å·®(MSE)"]:.6f}', 'Numerical Difference Degree'],
        ['Mean Absolute Error', f'{metrics["å¹³å‡ç»å¯¹è¯¯å·®(MAE)"]:.6f}', 'Average Deviation'],
        ['', '', ''],
        ['Overall Assessment', quality, quality_desc]
    ]
    table_title = 'Detailed Evaluation Report'
    
    table = ax4.table(cellText=table_data, cellLoc='center', loc='center',
                      colWidths=[0.3, 0.35, 0.35])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    for i in range(len(table_data)):
        for j in range(3):
            cell = table[(i, j)]
            if i == 0:  # æ ‡é¢˜è¡Œ
                cell.set_facecolor('#4472C4')
                cell.set_text_props(weight='bold', color='white')
            elif i == len(table_data) - 1:  # è¯„ä¼°è¡Œ
                cell.set_facecolor('#E7E6E6')
                cell.set_text_props(weight='bold')
            else:
                cell.set_facecolor('#F2F2F2')
    
    ax4.set_title(table_title, fontweight='bold', pad=20)
    
    plt.tight_layout()
    return fig

def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ YOLOv5 DXNN vs YOLOv5SU ç›¸ä¼¼åº¦åˆ†æž")
    print("="*50)
    
    # åŠ è½½é…ç½®å’Œæ¨¡åž‹
    config_path = './yolov5su.json'
    with open(config_path, "r") as f:
        config = json.load(f)
    
    # DXNNæ¨¡åž‹
    ie = InferenceEngine(config["model"]["path"])
    print(f"âœ… DXNNæ¨¡åž‹å·²åŠ è½½")
    
    # YOLOv5SUæ¨¡åž‹
    from ultralytics import YOLO
    yolo_model = YOLO('./model/yolov5su.pt')
    print(f"âœ… YOLOv5SUæ¨¡åž‹å·²åŠ è½½")
    
    # åŠ è½½æµ‹è¯•å›¾ç‰‡
    test_image = "./test.jpg"
    original_img = cv2.imread(test_image)
    preprocessed_img, _, _ = letterbox_resize(original_img, (640, 640))
    
    print(f"ðŸ“· æµ‹è¯•å›¾ç‰‡: {os.path.basename(test_image)}")
    
    # DXNNæŽ¨ç†
    dxnn_input = cv2.cvtColor(preprocessed_img, cv2.COLOR_BGR2RGB)
    dxnn_outputs = ie.run([dxnn_input])
    dxnn_main = dxnn_outputs[0]
    
    # YOLOv5SUæŽ¨ç†
    yolo_input_rgb = cv2.cvtColor(preprocessed_img, cv2.COLOR_BGR2RGB)
    yolo_input_tensor = torch.from_numpy(yolo_input_rgb).float().permute(2, 0, 1).unsqueeze(0) / 255.0
    with torch.no_grad():
        raw_output = yolo_model.model(yolo_input_tensor)
    yolo_main = raw_output[0].cpu().numpy()
    
    print(f"âœ… æŽ¨ç†å®Œæˆ")
    print(f"   DXNNè¾“å‡º: {dxnn_main.shape}")
    print(f"   YOLOv5SUè¾“å‡º: {yolo_main.shape}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡
    print("ðŸ“Š è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡...")
    metrics = calculate_similarity_metrics(dxnn_main, yolo_main)
    
    # æ‰“å°ç»“æžœ
    print("\nðŸ“ˆ ç›¸ä¼¼åº¦åˆ†æžç»“æžœ:")
    print("-" * 50)
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.6f}")
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    print("ðŸŽ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    fig = create_similarity_visualization(metrics)
    
    # ä¿å­˜å›¾è¡¨
    output_path = "./similarity_analysis.png"
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… åˆ†æžå®Œæˆ!")
    print(f"ðŸ“„ å›¾è¡¨å·²ä¿å­˜: {output_path}")

if __name__ == "__main__":
    main()